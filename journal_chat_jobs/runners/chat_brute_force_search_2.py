"""
Second version of chat brute force search: rank all the journeys on a 1-10
scale using gpt4o-mini (released the same day this was written). Then, for
those within 2 of the top score, compare them pairwise to find the best one
"""

import asyncio
from dataclasses import dataclass
import json
import math
import random
from typing import (
    AsyncIterable,
    Awaitable,
    Callable,
    List,
    Literal,
    Optional,
    Set,
    Tuple,
    TypeVar,
    Union,
    cast,
)
from error_middleware import handle_warning
from itgs import Itgs
from journal_chat_jobs.lib.journal_chat_job_context import JournalChatJobContext
from lib.image_files.image_file_ref import ImageFileRef
from lib.journals.journal_chat_redis_packet import (
    EventBatchPacketDataItemDataError,
)
from lib.journals.journal_entry_item_data import (
    JournalEntryItemData,
    JournalEntryItemDataClient,
    JournalEntryItemDataDataTextual,
    JournalEntryItemDataDataTextualClient,
    JournalEntryItemTextualPartJourney,
    JournalEntryItemTextualPartJourneyClient,
    JournalEntryItemTextualPartJourneyClientDetails,
    JournalEntryItemTextualPartParagraph,
    MinimalJourneyInstructor,
)
from lib.transcripts.cache import get_transcript

from lib.transcripts.model import Transcript
from lib.journals.journal_stats import JournalStats
import openai
import os
import lib.users.entitlements
import lib.image_files.auth
import journal_chat_jobs.lib.chat_helper as chat_helper


TECHNIQUE_PARAMETERS = cast(
    chat_helper.TechniqueParameters,
    {
        "type": "llm",
        "platform": "openai",
        "model": "gpt-4o",
    },
)
SMALL_MODEL = "gpt-4o-mini"

BIG_RATELIMIT_CATEGORY = "gpt-4o"
SMALL_RATELIMIT_CATEGORY = "gpt-4o-mini"
CONCURRENCY = 1  # This spends most of its time ratelimited anyway



async def handle_chat(itgs: Itgs, ctx: JournalChatJobContext) -> None:
    """The second iteration of chat brute force search; much faster and cheaper

    When `replace_index` is None, this responds to the conversation of the the
    current point, where presumably the last message in the conversation is from
    the user.

    Alternatively, if `replace_index` is set, replaces the message at that index
    with a new response from the system. This will only work well if the item being
    replaced was also generated by this function
    """
    await chat_helper.handle_chat_outer_loop(
        itgs,
        ctx,
        technique_parameters=TECHNIQUE_PARAMETERS,
        response_pipeline=_response_pipeline,
        prompt_identifier="chat_brute_force_search_2",
    )


async def _response_pipeline(
    itgs: Itgs,
    /,
    *,
    ctx: JournalChatJobContext,
    greeting: JournalEntryItemData,
    user_message: JournalEntryItemData,
    stats: JournalStats,
) -> AsyncIterable[
    Union[
        Tuple[bool, JournalEntryItemData, JournalEntryItemDataClient],
        Tuple[Literal[None], EventBatchPacketDataItemDataError],
    ]
]:
    await chat_helper.publish_spinner(itgs, ctx=ctx, message="Running prechecks...")
    text_greeting = chat_helper.extract_as_text(greeting)
    text_user_message = chat_helper.extract_as_text(user_message)

    client = openai.OpenAI(api_key=os.environ["OSEH_OPENAI_API_KEY"])
    try:
        # using openai.AsyncOpenAI breaks redis somehow... if you try to ping
        # redis after it, no matter what (even in a fresh connection), it will fail
        # with asyncio.CancelledError
        moderation_response = await asyncio.to_thread(
            client.moderations.create, input=text_user_message
        )
    except Exception as e:
        stats.incr_system_chats_failed_net_unknown(
            unix_date=ctx.queued_at_unix_date_in_stats_tz,
            **TECHNIQUE_PARAMETERS,
            prompt_identifier="moderation",
            category="net",
            detail="unknown",
            error_name=type(e).__name__,
        )
        yield None, EventBatchPacketDataItemDataError(
            type="error",
            message="Failed to connect with LLM",
            detail="moderation error",
        )
        return

    if moderation_response.results[0].categories.self_harm_intent:
        yield True, *chat_helper.get_message_from_paragraphs(
            [
                "In a crisis?",
                "Text HELLO to 741741 to connect with a volunteer Crisis Counselor",
                "Free 24/7 support at your fingerprints.",
                "*741741 is a registered trademark of Crisis Text Line, Inc.",
            ]
        )
        return

    if moderation_response.results[0].flagged:
        stats.incr_system_chats_failed_llm(
            unix_date=ctx.queued_at_unix_date_in_stats_tz,
            **TECHNIQUE_PARAMETERS,
            prompt_identifier="moderation",
            category="llm",
            detail="flagged",
        )
        yield None, EventBatchPacketDataItemDataError(
            type="error",
            message="Failed to create response",
            detail="flagged",
        )
        return

    pro_entitlement = await lib.users.entitlements.get_entitlement(
        itgs, user_sub=ctx.user_sub, identifier="pro"
    )
    has_pro = pro_entitlement is not None and pro_entitlement.is_active

    await chat_helper.publish_spinner(
        itgs, ctx=ctx, message="Writing initial response..."
    )

    try:
        await chat_helper.reserve_tokens(
            itgs, ctx=ctx, category=BIG_RATELIMIT_CATEGORY, tokens=2048
        )
        chat_response = await asyncio.to_thread(
            client.chat.completions.create,
            messages=[
                {
                    "role": "system",
                    "content": """
Objective: Craft responses that acknowledge the user’s current feelings, validate their experience, and leave room to later offer class suggestions to help them achieve their desired state.

Write 2-3 sentences acknowledging the user’s current feelings. Use empathetic language to show understanding and support.
End your response such that it seems like you still have more to say.
                        """.strip(),
                },
                {"role": "assistant", "content": text_greeting},
                {"role": "user", "content": text_user_message},
            ],
            model=TECHNIQUE_PARAMETERS["model"],
            max_tokens=2048,
        )
    except Exception as e:
        if os.environ["ENVIRONMENT"] == "dev":
            await handle_warning(
                f"{__name__}:llm", f"Failed to connect with LLM", exc=e
            )
        stats.incr_system_chats_failed_net_unknown(
            unix_date=ctx.queued_at_unix_date_in_stats_tz,
            **TECHNIQUE_PARAMETERS,
            prompt_identifier="chat",
            category="net",
            detail="unknown",
            error_name=type(e).__name__,
        )
        yield None, EventBatchPacketDataItemDataError(
            type="error",
            message="Failed to connect with LLM",
            detail="empathy error",
        )
        return

    empathy_message = chat_response.choices[0].message
    if empathy_message.content is None:
        stats.incr_system_chats_failed_llm(
            unix_date=ctx.queued_at_unix_date_in_stats_tz,
            **TECHNIQUE_PARAMETERS,
            prompt_identifier="chat_initial_empathy",
            category="llm",
            detail="no content",
        )
        yield None, EventBatchPacketDataItemDataError(
            type="error",
            message="Failed to create response",
            detail="no content",
        )
        return

    yield False, *chat_helper.get_message_from_text(empathy_message.content)
    await chat_helper.publish_spinner(
        itgs, ctx=ctx, message="Searching for free journey..."
    )

    try:
        free_class, free_class_transcript = await select_class(
            itgs,
            ctx=ctx,
            user_message=user_message,
            stats=stats,
            pro=False,
            has_pro=has_pro,
        )
    except ValueError as e:
        yield None, EventBatchPacketDataItemDataError(
            type="error",
            message="Failed to select free journey",
            detail=str(e),
        )
        return

    try:
        await chat_helper.reserve_tokens(
            itgs, ctx=ctx, category=BIG_RATELIMIT_CATEGORY, tokens=2048
        )
        chat_response = await asyncio.to_thread(
            client.chat.completions.create,
            messages=[
                {
                    "role": "system",
                    "content": "You should always suggest 1-2 sentences to complete their response. Do not attempt to rewrite their existing response, and do not repeat content from their existing response.",
                },
                {
                    "role": "user",
                    "content": (
                        f"""
Finish my response to a user who wrote

{user_message}

I want to recommend them {free_class.title} by {free_class.instructor.name}. Here's
the transcript of that class

```txt
{str(free_class_transcript)}
```

I need you to add 1-2 sentences to my response so far. Do not include my response in your
message.

{empathy_message.content}
                    """
                    ),
                },
            ],
            model=TECHNIQUE_PARAMETERS["model"],
            max_tokens=2048,
        )
    except Exception as e:
        if os.environ["ENVIRONMENT"] == "dev":
            await handle_warning(
                f"{__name__}:llm", f"Failed to connect with LLM", exc=e
            )
        stats.incr_system_chats_failed_net_unknown(
            unix_date=ctx.queued_at_unix_date_in_stats_tz,
            **TECHNIQUE_PARAMETERS,
            prompt_identifier="chat_free_class",
            category="net",
            detail="unknown",
            error_name=type(e).__name__,
        )
        yield None, EventBatchPacketDataItemDataError(
            type="error",
            message="Failed to connect with LLM",
            detail="empathy error",
        )
        return

    free_class_message = chat_response.choices[0].message
    if free_class_message.content is None:
        stats.incr_system_chats_failed_llm(
            unix_date=ctx.queued_at_unix_date_in_stats_tz,
            **TECHNIQUE_PARAMETERS,
            prompt_identifier="chat_free_class",
            category="llm",
            detail="no content",
        )
        yield None, EventBatchPacketDataItemDataError(
            type="error",
            message="Failed to create response",
            detail="no content",
        )
        return

    paragraphs1 = empathy_message.content.split("\n")
    paragraphs1 = [p.strip() for p in paragraphs1]
    paragraphs1 = [p for p in paragraphs1 if p]
    paragraphs2 = free_class_message.content.split("\n")
    paragraphs2 = [p.strip() for p in paragraphs2]
    paragraphs2 = [p for p in paragraphs2 if p]
    paragraphs = paragraphs1 + paragraphs2
    yield True, JournalEntryItemData(
        type="chat",
        data=JournalEntryItemDataDataTextual(
            type="textual",
            parts=[
                *[
                    JournalEntryItemTextualPartParagraph(
                        type="paragraph",
                        value=p,
                    )
                    for p in paragraphs
                ],
                JournalEntryItemTextualPartJourney(
                    type="journey",
                    uid=free_class.uid,
                ),
            ],
        ),
        display_author="other",
    ), JournalEntryItemDataClient(
        type="chat",
        data=JournalEntryItemDataDataTextualClient(
            type="textual",
            parts=[
                *[
                    JournalEntryItemTextualPartParagraph(
                        type="paragraph",
                        value=p,
                    )
                    for p in paragraphs
                ],
                JournalEntryItemTextualPartJourneyClient(
                    details=free_class,
                    type="journey",
                    uid=free_class.uid,
                ),
            ],
        ),
        display_author="other",
    )


@dataclass
class PossibleJourney:
    journey_uid: str
    audio_file_uid: str
    journey_title: str
    journey_description: str
    instructor_name: str
    transcript_uid: str


async def select_class(
    itgs: Itgs,
    /,
    *,
    ctx: JournalChatJobContext,
    user_message: JournalEntryItemData,
    stats: JournalStats,
    pro: bool,
    has_pro: bool,
) -> Tuple[JournalEntryItemTextualPartJourneyClientDetails, Transcript]:
    conn = await itgs.conn()
    cursor = conn.cursor("none")

    responses = await cursor.executeunified3(
        (
            ("SELECT 1 FROM users WHERE sub=?", (ctx.user_sub,)),
            (
                """
WITH 
relevant_journeys(journey_id) AS (
SELECT j.id
FROM journeys AS j
WHERE
    j.deleted_at IS NULL
    AND (? = 0 OR NOT EXISTS (
        SELECT 1 FROM courses, course_journeys
        WHERE
            courses.id = course_journeys.course_id
            AND course_journeys.journey_id = j.id
            AND (courses.flags & 128) = 0
    ))
    AND (? = 0 OR EXISTS (
        SELECT 1 FROM courses, course_journeys
        WHERE
            courses.id = course_journeys.course_id
            AND course_journeys.journey_id = j.id
            AND (courses.flags & 256) <> 0
    ))
    AND j.special_category IS NULL
)
, user_journey_counts_raw(journey_id, cnt) AS (
SELECT
    uj.journey_id,
    COUNT(*)
FROM user_journeys AS uj
WHERE uj.user_id = (SELECT users.id FROM users WHERE users.sub=?)
)
, user_journey_counts(journey_id, cnt) AS (
SELECT
    relevant_journeys.journey_id,
    COALESCE(user_journey_counts_raw.cnt, 0)
FROM relevant_journeys
LEFT OUTER JOIN user_journey_counts_raw ON user_journey_counts_raw.journey_id = relevant_journeys.journey_id
)
SELECT 
    journeys.uid AS d1,
    content_files.uid AS d2,
    journeys.title AS d3,
    journeys.description AS d4,
    instructors.name AS d5,
    transcripts.uid AS d6
FROM 
    user_journey_counts, 
    journeys,
    content_files,
    instructors,
    content_file_transcripts,
    transcripts
WHERE
    user_journey_counts.journey_id = journeys.id
    AND user_journey_counts.cnt = (SELECT MIN(cnt) FROM user_journey_counts)
    AND content_files.id = journeys.audio_content_file_id
    AND content_file_transcripts.id = (
        SELECT cft.id FROM content_file_transcripts AS cft
        WHERE cft.content_file_id = content_files.id
        ORDER BY cft.created_at DESC, cft.uid ASC
    )
    AND content_file_transcripts.transcript_id = transcripts.id
    AND instructors.id = journeys.instructor_id
                """,
                [
                    int(not pro),
                    int(pro),
                    ctx.user_sub,
                ],
            ),
        )
    )

    if not responses[0].results:
        stats.incr_system_chats_failed_internal(
            unix_date=ctx.queued_at_unix_date_in_stats_tz,
            **TECHNIQUE_PARAMETERS,
            prompt_identifier=f"select_class_{'' if pro else 'non_'}pro:user_not_found",
            category="internal",
        )
        raise ValueError(f"User {ctx.user_sub} not found")

    if not responses[1].results:
        stats.incr_system_chats_failed_internal(
            unix_date=ctx.queued_at_unix_date_in_stats_tz,
            **TECHNIQUE_PARAMETERS,
            prompt_identifier=f"select_class_{'' if pro else 'non_'}pro:no_journeys",
            category="internal",
        )
        raise ValueError(f"No journeys found for user {ctx.user_sub}")

    possible_journeys: List[PossibleJourney] = []
    for row in responses[1].results:
        possible_journeys.append(
            PossibleJourney(
                journey_uid=row[0],
                audio_file_uid=row[1],
                journey_title=row[2],
                journey_description=row[3],
                instructor_name=row[4],
                transcript_uid=row[5],
            )
        )

    await chat_helper.publish_pbar(
        itgs,
        ctx=ctx,
        message=f"Rating {len(possible_journeys)} journeys...",
        at=0,
        of=len(possible_journeys),
    )

    async def get_possible_journey_rating(
        possible_journey: PossibleJourney,
    ) -> Optional[Tuple[float, PossibleJourney]]:
        transcript = await get_transcript(itgs, uid=possible_journey.transcript_uid)
        if transcript is None:
            return None

        text_transcript = str(transcript.to_internal())
        await chat_helper.reserve_tokens(
            itgs, ctx=ctx, category=SMALL_RATELIMIT_CATEGORY, tokens=2048
        )
        rating_response = await asyncio.to_thread(
            client.chat.completions.create,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "The user provides a message and a journey, and you rate how well "
                        "the journey fits the message from 1 to 10, where 1 means the journey "
                        "would be counterproductive (such as a sleep class when they want to feel awake), "
                        "5 means the journey is unrelated (like a cooking class when they want to talk about their feelings), "
                        "and 10 means the journey is exactly what they need (like a guide to calm when they are feeling anxious)."
                    ),
                },
                {
                    "role": "user",
                    "content": f"""The message is:

```txt
{user_message_text}
```

The journey is:

title: {possible_journey.journey_title}
description: {possible_journey.journey_description}
instructor: {possible_journey.instructor_name}
transcript:

```txt
{text_transcript}
```
""",
                },
            ],
            model=SMALL_MODEL,
            tools=[
                {
                    "type": "function",
                    "function": {
                        "name": "rate_journey",
                        "description": "Apply the given rating to the journey",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "rating": {
                                    "type": "number",
                                    "description": "The journeys rating from 1 (inclusive) to 10 (inclusive)",
                                    "min": 1,
                                    "max": 10,
                                }
                            },
                            "required": ["rating"],
                        },
                    },
                }
            ],
            tool_choice={"type": "function", "function": {"name": "rate_journey"}},
            max_tokens=2048,
        )

        if not rating_response.choices:
            return None

        rating_message = rating_response.choices[0].message
        if not rating_message.tool_calls or len(rating_message.tool_calls) != 1:
            return None

        rating_arguments_json = rating_message.tool_calls[0].function.arguments
        try:
            rating_arguments = json.loads(rating_arguments_json)
        except Exception:
            return None

        if not isinstance(rating_arguments, dict):
            return None

        rating = rating_arguments.get("rating")
        if not isinstance(rating, (int, float)) or rating < 1 or rating > 10:
            return None

        return rating, possible_journey

    user_message_text = chat_helper.extract_as_text(user_message)
    rated_journeys: List[Tuple[float, PossibleJourney]] = []
    client = openai.OpenAI(api_key=os.environ["OSEH_OPENAI_API_KEY"])

    running: Set[asyncio.Task] = set()
    concurrency_limit = CONCURRENCY
    next_index = 0
    finished = 0
    _pbar_task: Optional[asyncio.Task] = None

    while running or (next_index < len(possible_journeys)):
        while len(running) < concurrency_limit and next_index < len(possible_journeys):
            running.add(
                asyncio.create_task(
                    get_possible_journey_rating(possible_journeys[next_index])
                )
            )
            next_index += 1

        done, running = await asyncio.wait(running, return_when=asyncio.FIRST_COMPLETED)
        for task in done:
            result = task.result()
            if result is not None:
                rated_journeys.append(result)

        finished += len(done)
        if _pbar_task is not None and _pbar_task.done():
            await _pbar_task
            _pbar_task = None
        if _pbar_task is None:
            _pbar_task = asyncio.create_task(
                chat_helper.publish_pbar(
                    itgs,
                    ctx=ctx,
                    message=f"Rating {len(possible_journeys)} journeys...",
                    at=finished,
                    of=len(possible_journeys),
                )
            )

    if _pbar_task is not None:
        await _pbar_task
    del _pbar_task

    if not rated_journeys:
        stats.incr_system_chats_failed_internal(
            unix_date=ctx.queued_at_unix_date_in_stats_tz,
            **TECHNIQUE_PARAMETERS,
            prompt_identifier=f"select_class_{'' if pro else 'non_'}pro:no_rated_journeys",
            category="internal",
        )
        raise ValueError(f"No rated journeys found for user {ctx.user_sub}")

    best_rating = max(rated_journeys, key=lambda x: x[0])[0]
    eligible_journeys = [
        journey for rating, journey in rated_journeys if rating >= best_rating - 2
    ]

    async def _compare(a: PossibleJourney, b: PossibleJourney) -> int:
        """Compares two possible journeys using the llm"""
        transcript1 = await get_transcript(itgs, a.transcript_uid)
        transcript2 = await get_transcript(itgs, b.transcript_uid)

        if transcript1 is None and transcript2 is None:
            return random.choice([-1, 1])
        if transcript1 is None:
            return 1
        if transcript2 is None:
            return -1

        a_id = chat_helper.make_id(a.journey_title)
        b_id = chat_helper.make_id(b.journey_title)

        if a_id == b_id or a_id == "" or b_id == "":
            a_id = a.journey_uid
            b_id = b.journey_uid

        if a_id == b_id:
            return random.choice([-1, 1])

        await chat_helper.reserve_tokens(
            itgs, ctx=ctx, category=SMALL_RATELIMIT_CATEGORY, tokens=2048
        )
        comparison_response = await asyncio.to_thread(
            client.chat.completions.create,
            messages=[
                {
                    "role": "system",
                    "content": (
                        "The user provides a message and two journeys. You determine which journey is a better "
                        "fit for the message. A better fit is one that is more relevant to the user's needs."
                    ),
                },
                {
                    "role": "user",
                    "content": f"""The message is:

```txt
{user_message_text}
```

The first journey is:

id: {a_id}
title: {a.journey_title}
description: {a.journey_description}
instructor: {a.instructor_name}
transcript:

```txt
{str(transcript1.to_internal())}
```

The second journey is:

id: {b_id}
title: {b.journey_title}
description: {b.journey_description}
instructor: {b.instructor_name}
transcript:

```txt
{str(transcript2.to_internal())}
```
""",
                },
            ],
            model=SMALL_MODEL,
            tools=[
                {
                    "type": "function",
                    "function": {
                        "name": "select_journey",
                        "description": "Select the given journey as the preferred journey",
                        "parameters": {
                            "type": "object",
                            "properties": {
                                "id": {
                                    "type": "string",
                                    "description": "The ID of the journey which is preferred amongst the two",
                                }
                            },
                            "required": ["id"],
                        },
                    },
                }
            ],
            tool_choice={"type": "function", "function": {"name": "select_journey"}},
            max_tokens=2048,
        )

        if not comparison_response.choices:
            return random.choice([-1, 1])

        comparison_message = comparison_response.choices[0].message
        if not comparison_message.tool_calls or len(comparison_message.tool_calls) != 1:
            return random.choice([-1, 1])

        comparison_arguments_json = comparison_message.tool_calls[0].function.arguments
        try:
            comparison_arguments = json.loads(comparison_arguments_json)
        except Exception:
            return random.choice([-1, 1])

        if not isinstance(comparison_arguments, dict):
            return random.choice([-1, 1])

        selected_id = comparison_arguments.get("id")
        if not isinstance(selected_id, str):
            return random.choice([-1, 1])

        if selected_id == a_id:
            return -1
        if selected_id == b_id:
            return 1
        return random.choice([-1, 1])

    num_comparisons = 0
    _spinner_task = cast(Optional[asyncio.Task], None)

    async def _compare_with_progress(a: PossibleJourney, b: PossibleJourney) -> int:
        nonlocal num_comparisons, _spinner_task

        result = await _compare(a, b)
        num_comparisons += 1
        if _spinner_task is not None and _spinner_task.done():
            await _spinner_task
            _spinner_task = None
        if _spinner_task is None:
            _spinner_task = asyncio.create_task(
                chat_helper.publish_spinner(
                    itgs,
                    ctx=ctx,
                    message=f"Ranking top {len(eligible_journeys)} pairwise...",
                    detail=f"Comparisons so far: {num_comparisons}",
                )
            )
        return result

    best_journey = await fast_top_1(
        eligible_journeys,
        compare=_compare_with_progress,
        semaphore=asyncio.Semaphore(CONCURRENCY),
    )
    if _spinner_task is not None:
        await _spinner_task
    del _spinner_task

    await chat_helper.publish_spinner(
        itgs, ctx=ctx, message=f"Finishing up (total comparisons: {num_comparisons})..."
    )

    response = await cursor.execute(
        """
SELECT
    journeys.uid,
    journeys.title,
    journeys.description,
    darkened_image_files.uid,
    content_files.duration_seconds,
    instructors.name,
    instructor_profile_pictures.uid,
    (SELECT MAX(uj.created_at) FROM user_journeys AS uj WHERE uj.journey_id = journeys.id AND uj.user_id = (SELECT users.id FROM users WHERE users.sub=?)) AS last_taken_at,
    (SELECT ul.created_at FROM user_likes AS ul WHERE ul.journey_id = journeys.id AND ul.user_id = (SELECT users.id FROM users WHERE users.sub=?)) AS liked_at
FROM 
    journeys, 
    image_files AS darkened_image_files, 
    content_files, 
    instructors
LEFT OUTER JOIN image_files AS instructor_profile_pictures ON instructor_profile_pictures.id = instructors.picture_image_file_id
WHERE
    journeys.uid = ?
    AND journeys.deleted_at IS NULL
    AND darkened_image_files.id = journeys.darkened_background_image_file_id
    AND content_files.id = journeys.audio_content_file_id
    AND instructors.id = journeys.instructor_id
        """,
        (ctx.user_sub, ctx.user_sub, best_journey.journey_uid),
    )
    if not response.results:
        stats.incr_system_chats_failed_internal(
            unix_date=ctx.queued_at_unix_date_in_stats_tz,
            **TECHNIQUE_PARAMETERS,
            prompt_identifier=f"select_class_{'' if pro else 'non_'}pro:journey_not_found",
            category="internal",
        )
        raise ValueError(f"Journey {best_journey.journey_uid} not found")

    row = response.results[0]
    access = (
        "free"
        if not pro
        else ("paid-requires-upgrade" if not has_pro else "paid-unlocked")
    )

    transcript = await get_transcript(itgs, best_journey.transcript_uid)
    if transcript is None:
        stats.incr_system_chats_failed_internal(
            unix_date=ctx.queued_at_unix_date_in_stats_tz,
            **TECHNIQUE_PARAMETERS,
            prompt_identifier=f"select_class_{'' if pro else 'non_'}pro:transcript_not_found",
            category="internal",
        )
        raise ValueError(
            f"Transcript for journey {best_journey.journey_uid} not found after selection"
        )

    return (
        JournalEntryItemTextualPartJourneyClientDetails(
            uid=row[0],
            title=row[1],
            description=row[2],
            darkened_background=ImageFileRef(
                uid=row[3],
                jwt=await lib.image_files.auth.create_jwt(
                    itgs,
                    image_file_uid=row[3],
                ),
            ),
            duration_seconds=row[4],
            instructor=MinimalJourneyInstructor(
                name=row[5],
                image=(
                    None
                    if row[6] is None
                    else ImageFileRef(
                        uid=row[6],
                        jwt=await lib.image_files.auth.create_jwt(
                            itgs,
                            image_file_uid=row[6],
                        ),
                    )
                ),
            ),
            last_taken_at=row[7],
            liked_at=row[8],
            access=access,
        ),
        transcript.to_internal(),
    )


T = TypeVar("T")
V = TypeVar("V")
Q = TypeVar("Q")


async def fast_top_1(
    arr: List[T],
    /,
    *,
    compare: Callable[[T, T], Awaitable[int]],
    semaphore: asyncio.Semaphore,
) -> T:
    """Finds the best possible candidate from the given list, assuming that
    the comparison function may be inconsistent (i.e., it may not result
    in a valid partial ordering)

    By construction, it does not repeat known comparisons
    """
    if len(arr) <= 6:  # use 2 for fastest, larger for more accurate
        return await _top_1_pairwise_full(arr, compare=compare, semaphore=semaphore)

    num_subgroups = max(math.ceil(math.sqrt(len(arr))), 2)
    subgroup_size = len(arr) // num_subgroups

    subgroups: List[List[T]] = []
    for i in range(num_subgroups):
        subgroups.append(arr[i * subgroup_size : (i + 1) * subgroup_size])

    best_of_subgroups = await asyncio.gather(
        *[
            fast_top_1(subgroup, compare=compare, semaphore=semaphore)
            for subgroup in subgroups
        ]
    )
    return await fast_top_1(best_of_subgroups, compare=compare, semaphore=semaphore)


async def _top_1_pairwise_full(
    arr: List[T],
    /,
    *,
    compare: Callable[[T, T], Awaitable[int]],
    semaphore: asyncio.Semaphore,
) -> T:
    """Finds the best option amongst the given list using n choose 2 comparisons (so for 6
    options, takes 15 comparisons). All comparisons can be done in parallel if there is enough
    space on the semaphore
    """
    if not arr:
        raise ValueError("Cannot find the best of an empty list")

    if len(arr) == 1:
        return arr[0]

    comparisons: List[List[int]] = [[0] * len(arr) for _ in range(len(arr))]

    running: Set[asyncio.Task] = set()
    # each returns [(i, j), result]

    i = 0
    j = 1
    additional_acquisitions = 0
    while running or i < len(arr) - 1:
        # start as many as we can
        while (additional_acquisitions > 0 or not semaphore.locked()) and i < len(
            arr
        ) - 1:
            if additional_acquisitions > 0:
                additional_acquisitions -= 1
            else:
                await semaphore.acquire()

            running.add(
                asyncio.create_task(
                    _augmented_compare((i, j), arr[i], arr[j], compare=compare)
                )
            )
            j += 1
            if j >= len(arr):
                i += 1
                j = i + 1

        while additional_acquisitions > 0:
            semaphore.release()
            additional_acquisitions -= 1

        if not running:
            await semaphore.acquire()
            additional_acquisitions += 1
            continue

        if i >= len(arr) - 1:
            done, running = await asyncio.wait(
                running, return_when=asyncio.FIRST_COMPLETED
            )
            for t in done:
                semaphore.release()
                (done_i, done_j), done_comparison = t.result()
                comparisons[done_i][done_j] = done_comparison
                comparisons[done_j][done_i] = -done_comparison
            continue

        something_finished_task = asyncio.create_task(
            asyncio.wait(running, return_when=asyncio.FIRST_COMPLETED)
        )
        acquire_task = asyncio.create_task(semaphore.acquire())
        await asyncio.wait(
            [something_finished_task, acquire_task], return_when=asyncio.FIRST_COMPLETED
        )

        if not acquire_task.cancel():
            acquire_task.result()
            additional_acquisitions += 1

        if not something_finished_task.cancel():
            done, running = await something_finished_task
            for t in done:
                semaphore.release()
                (done_i, done_j), done_comparison = t.result()
                comparisons[done_i][done_j] = done_comparison
                comparisons[done_j][done_i] = -done_comparison

    while additional_acquisitions > 0:
        semaphore.release()
        additional_acquisitions -= 1

    scores = [sum(row) for row in comparisons]
    best_index = max(range(len(arr)), key=lambda i: scores[i])
    return arr[best_index]


async def _augmented_compare(
    retval1: V, a: T, b: T, compare: Callable[[T, T], Awaitable[int]]
) -> Tuple[V, int]:
    return retval1, await compare(a, b)
